{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f95493",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 43) (3265751536.py, line 43)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mWe'll use K-Means to identify outliers based on distance from cluster centroids.\u001b[39m\n      ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 43)\n"
     ]
    }
   ],
   "source": [
    "# Part A: Outlier Detection Methods\n",
    "\n",
    "## Method 1: Partitioning Method (K-Means Based)\n",
    "\n",
    "### Code Cell 1: Import Libraries and Load Data\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import cdist\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('reduced_file.csv')\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()\n",
    "```\n",
    "\n",
    "### Text Cell 1: Data Preprocessing\n",
    "Before detecting outliers, we need to prepare numerical features and handle missing values.\n",
    "\n",
    "### Code Cell 2: Prepare Numerical Features\n",
    "```python\n",
    "# Handle missing values in TotalCharges\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)\n",
    "\n",
    "# Select numerical features for outlier detection\n",
    "numerical_features = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "X_numerical = df[numerical_features].copy()\n",
    "\n",
    "print(\"Numerical features statistics:\")\n",
    "print(X_numerical.describe())\n",
    "```\n",
    "\n",
    "### Text Cell 2: K-Means Clustering for Outlier Detection\n",
    "We'll use K-Means to identify outliers based on distance from cluster centroids.\n",
    "Outliers are points that are far from their assigned cluster center.\n",
    "\n",
    "### Code Cell 3: K-Means Outlier Detection\n",
    "```python\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_numerical)\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Calculate distances to cluster centers\n",
    "distances = cdist(X_scaled, kmeans.cluster_centers_, 'euclidean')\n",
    "min_distances = np.min(distances, axis=1)\n",
    "\n",
    "# Define outliers as points beyond 95th percentile distance\n",
    "threshold_kmeans = np.percentile(min_distances, 95)\n",
    "outliers_kmeans = min_distances > threshold_kmeans\n",
    "\n",
    "print(f\"K-Means Method:\")\n",
    "print(f\"  Outliers detected: {outliers_kmeans.sum()}\")\n",
    "print(f\"  Percentage: {outliers_kmeans.sum()/len(df)*100:.2f}%\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X_numerical['tenure'], X_numerical['MonthlyCharges'], \n",
    "            c=outliers_kmeans, cmap='coolwarm', alpha=0.6)\n",
    "plt.xlabel('Tenure')\n",
    "plt.ylabel('Monthly Charges')\n",
    "plt.title('K-Means Outliers: Tenure vs Monthly Charges')\n",
    "plt.colorbar(label='Outlier')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(X_numerical['tenure'], X_numerical['TotalCharges'], \n",
    "            c=outliers_kmeans, cmap='coolwarm', alpha=0.6)\n",
    "plt.xlabel('Tenure')\n",
    "plt.ylabel('Total Charges')\n",
    "plt.title('K-Means Outliers: Tenure vs Total Charges')\n",
    "plt.colorbar(label='Outlier')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(min_distances, bins=50, alpha=0.7)\n",
    "plt.axvline(threshold_kmeans, color='r', linestyle='--', label='Threshold')\n",
    "plt.xlabel('Distance to Nearest Centroid')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distance Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Method 2: Hierarchical Clustering\n",
    "\n",
    "### Text Cell 3: Hierarchical Clustering Approach\n",
    "Hierarchical clustering builds a tree of clusters. We'll identify outliers as small clusters\n",
    "or points that merge late in the hierarchy.\n",
    "\n",
    "### Code Cell 4: Hierarchical Outlier Detection\n",
    "```python\n",
    "# Perform hierarchical clustering\n",
    "linkage_matrix = linkage(X_scaled, method='ward')\n",
    "\n",
    "# Cut tree to form clusters\n",
    "n_clusters = 5\n",
    "hierarchical_labels = fcluster(linkage_matrix, n_clusters, criterion='maxclust')\n",
    "\n",
    "# Identify small clusters as outliers\n",
    "cluster_counts = pd.Series(hierarchical_labels).value_counts()\n",
    "small_clusters = cluster_counts[cluster_counts < len(df) * 0.05].index\n",
    "outliers_hierarchical = np.isin(hierarchical_labels, small_clusters)\n",
    "\n",
    "print(f\"\\nHierarchical Method:\")\n",
    "print(f\"  Outliers detected: {outliers_hierarchical.sum()}\")\n",
    "print(f\"  Percentage: {outliers_hierarchical.sum()/len(df)*100:.2f}%\")\n",
    "print(f\"  Cluster sizes: {cluster_counts.sort_index().values}\")\n",
    "\n",
    "# Visualize dendrogram\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "dendrogram(linkage_matrix, truncate_mode='lastp', p=30)\n",
    "plt.xlabel('Sample Index or Cluster Size')\n",
    "plt.ylabel('Distance')\n",
    "plt.title('Hierarchical Clustering Dendrogram (Truncated)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_numerical['tenure'], X_numerical['MonthlyCharges'], \n",
    "            c=outliers_hierarchical, cmap='coolwarm', alpha=0.6)\n",
    "plt.xlabel('Tenure')\n",
    "plt.ylabel('Monthly Charges')\n",
    "plt.title('Hierarchical Outliers: Tenure vs Monthly Charges')\n",
    "plt.colorbar(label='Outlier')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Method 3: Density-Based Method (DBSCAN)\n",
    "\n",
    "### Text Cell 4: DBSCAN for Outlier Detection\n",
    "DBSCAN identifies outliers as points in low-density regions (labeled as -1).\n",
    "It's particularly effective for finding irregularly shaped clusters.\n",
    "\n",
    "### Code Cell 5: DBSCAN Outlier Detection\n",
    "```python\n",
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=10)\n",
    "dbscan_labels = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "# Points labeled as -1 are outliers\n",
    "outliers_dbscan = dbscan_labels == -1\n",
    "\n",
    "print(f\"\\nDBSCAN Method:\")\n",
    "print(f\"  Outliers detected: {outliers_dbscan.sum()}\")\n",
    "print(f\"  Percentage: {outliers_dbscan.sum()/len(df)*100:.2f}%\")\n",
    "print(f\"  Number of clusters found: {len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X_numerical['tenure'], X_numerical['MonthlyCharges'], \n",
    "            c=dbscan_labels, cmap='viridis', alpha=0.6)\n",
    "plt.xlabel('Tenure')\n",
    "plt.ylabel('Monthly Charges')\n",
    "plt.title('DBSCAN Clusters')\n",
    "plt.colorbar(label='Cluster')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(X_numerical['tenure'], X_numerical['MonthlyCharges'], \n",
    "            c=outliers_dbscan, cmap='coolwarm', alpha=0.6)\n",
    "plt.xlabel('Tenure')\n",
    "plt.ylabel('Monthly Charges')\n",
    "plt.title('DBSCAN Outliers')\n",
    "plt.colorbar(label='Outlier')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "# Compare all three methods\n",
    "comparison = pd.DataFrame({\n",
    "    'K-Means': outliers_kmeans,\n",
    "    'Hierarchical': outliers_hierarchical,\n",
    "    'DBSCAN': outliers_dbscan\n",
    "})\n",
    "comparison.sum().plot(kind='bar', color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "plt.ylabel('Number of Outliers')\n",
    "plt.title('Comparison of Outlier Detection Methods')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Consensus Outliers and Data Cleaning\n",
    "\n",
    "### Text Cell 5: Finding Consensus Outliers\n",
    "We'll identify outliers detected by multiple methods for more reliable detection.\n",
    "\n",
    "### Code Cell 6: Consensus and Removal\n",
    "```python\n",
    "# Create consensus outliers (detected by at least 2 methods)\n",
    "outlier_votes = (outliers_kmeans.astype(int) + \n",
    "                 outliers_hierarchical.astype(int) + \n",
    "                 outliers_dbscan.astype(int))\n",
    "\n",
    "consensus_outliers = outlier_votes >= 2\n",
    "\n",
    "print(\"\\nConsensus Analysis:\")\n",
    "print(f\"  Outliers detected by all 3 methods: {(outlier_votes == 3).sum()}\")\n",
    "print(f\"  Outliers detected by 2+ methods: {consensus_outliers.sum()}\")\n",
    "print(f\"  Percentage: {consensus_outliers.sum()/len(df)*100:.2f}%\")\n",
    "\n",
    "# Create cleaned dataset\n",
    "df_cleaned = df[~consensus_outliers].copy()\n",
    "print(f\"\\nDataset size after removing outliers:\")\n",
    "print(f\"  Original: {len(df)} rows\")\n",
    "print(f\"  Cleaned: {len(df_cleaned)} rows\")\n",
    "print(f\"  Removed: {len(df) - len(df_cleaned)} rows\")\n",
    "\n",
    "# Save cleaned dataset\n",
    "df_cleaned.to_csv('churn_cleaned.csv', index=False)\n",
    "print(\"\\nCleaned dataset saved as 'churn_cleaned.csv'\")\n",
    "\n",
    "# Visualize consensus\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_numerical['tenure'], X_numerical['MonthlyCharges'], \n",
    "            c=outlier_votes, cmap='RdYlGn_r', alpha=0.6, s=50)\n",
    "plt.colorbar(label='Number of Methods Detecting as Outlier')\n",
    "plt.xlabel('Tenure (months)')\n",
    "plt.ylabel('Monthly Charges ($)')\n",
    "plt.title('Consensus Outlier Detection\\n(0=Normal, 3=All methods agree)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0851ab",
   "metadata": {},
   "source": [
    "PART B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "509fccee",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 163) (733108715.py, line 163)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 163\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m2. **Robustness**: Reduces risk of overfitting to one model's bias\u001b[39m\n                                                               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 163)\n"
     ]
    }
   ],
   "source": [
    "# Part B: Strategy to Enhance Performance\n",
    "\n",
    "## Strategy Overview\n",
    "\n",
    "### Text Cell 6: Performance Enhancement Strategy\n",
    "**Strategy: Feature Engineering + Class Balancing + Ensemble Methods**\n",
    "\n",
    "This strategy combines three powerful techniques:\n",
    "1. **Feature Engineering**: Create meaningful features from existing data\n",
    "2. **Class Balancing**: Address imbalanced churn rates using SMOTE\n",
    "3. **Ensemble Methods**: Use multiple models for robust predictions\n",
    "\n",
    "### Code Cell 7: Feature Engineering\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load cleaned dataset\n",
    "df_clean = pd.read_csv('churn_cleaned.csv')\n",
    "\n",
    "# Feature Engineering\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Revenue per month of tenure\n",
    "    df['RevenuePerMonth'] = df['TotalCharges'] / (df['tenure'] + 1)\n",
    "    \n",
    "    # 2. Service count (number of additional services)\n",
    "    service_cols = ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection', \n",
    "                    'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
    "    df['ServiceCount'] = (df[service_cols] == 'Yes').sum(axis=1)\n",
    "    \n",
    "    # 3. Has premium services\n",
    "    df['HasPremiumServices'] = (df['ServiceCount'] >= 3).astype(int)\n",
    "    \n",
    "    # 4. Contract risk score (month-to-month = high risk)\n",
    "    df['ContractRisk'] = (df['Contract'] == 'Month-to-month').astype(int)\n",
    "    \n",
    "    # 5. Payment method risk\n",
    "    df['PaymentRisk'] = (df['PaymentMethod'] == 'Electronic check').astype(int)\n",
    "    \n",
    "    # 6. Senior with dependents\n",
    "    df['SeniorWithDependents'] = (df['SeniorCitizen'] == 1) & (df['Dependents'] == 'Yes')\n",
    "    df['SeniorWithDependents'] = df['SeniorWithDependents'].astype(int)\n",
    "    \n",
    "    # 7. Tenure category\n",
    "    df['TenureCategory'] = pd.cut(df['tenure'], bins=[0, 12, 24, 48, 100],\n",
    "                                   labels=['New', 'Medium', 'Long', 'VeryLong'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_engineered = engineer_features(df_clean)\n",
    "print(\"New features created:\")\n",
    "print(\"  - RevenuePerMonth: Average revenue per tenure month\")\n",
    "print(\"  - ServiceCount: Number of additional services\")\n",
    "print(\"  - HasPremiumServices: Binary flag for 3+ services\")\n",
    "print(\"  - ContractRisk: Month-to-month contract flag\")\n",
    "print(\"  - PaymentRisk: Electronic check payment flag\")\n",
    "print(\"  - SeniorWithDependents: Senior citizen with dependents\")\n",
    "print(\"  - TenureCategory: Categorical tenure grouping\")\n",
    "```\n",
    "\n",
    "### Text Cell 7: Why This Strategy Works\n",
    "**Rationale for Feature Engineering:**\n",
    "- **RevenuePerMonth**: Captures customer value intensity\n",
    "- **ServiceCount**: Measures engagement with company services\n",
    "- **Risk Scores**: Identify high-churn risk factors from domain knowledge\n",
    "- **Tenure Categories**: Non-linear relationships with churn\n",
    "\n",
    "**Benefits:**\n",
    "- Captures domain knowledge\n",
    "- Creates non-linear relationships\n",
    "- Improves model interpretability\n",
    "\n",
    "### Code Cell 8: Prepare Data for Modeling\n",
    "```python\n",
    "# Encode categorical variables\n",
    "df_model = df_engineered.copy()\n",
    "\n",
    "# Label encode binary categories\n",
    "binary_cols = ['gender', 'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling']\n",
    "le = LabelEncoder()\n",
    "for col in binary_cols:\n",
    "    df_model[col] = le.fit_transform(df_model[col])\n",
    "\n",
    "# One-hot encode multi-class categories\n",
    "categorical_cols = ['MultipleLines', 'InternetService', 'OnlineSecurity', \n",
    "                    'OnlineBackup', 'DeviceProtection', 'TechSupport',\n",
    "                    'StreamingTV', 'StreamingMovies', 'Contract', 'PaymentMethod',\n",
    "                    'TenureCategory']\n",
    "\n",
    "df_model = pd.get_dummies(df_model, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Encode target\n",
    "df_model['Churn'] = (df_model['Churn'] == 'Yes').astype(int)\n",
    "\n",
    "# Drop customerID\n",
    "df_model = df_model.drop('customerID', axis=1)\n",
    "\n",
    "print(f\"Final feature count: {df_model.shape[1] - 1}\")\n",
    "print(f\"Churn distribution:\\n{df_model['Churn'].value_counts()}\")\n",
    "```\n",
    "\n",
    "### Text Cell 8: Class Balancing with SMOTE\n",
    "**Why SMOTE?**\n",
    "- Synthetic Minority Over-sampling Technique\n",
    "- Generates synthetic examples of minority class\n",
    "- Better than simple over-sampling (avoids exact duplicates)\n",
    "- Helps model learn minority class patterns\n",
    "\n",
    "### Code Cell 9: Apply SMOTE and Train Models\n",
    "```python\n",
    "# Split features and target\n",
    "X = df_model.drop('Churn', axis=1)\n",
    "y = df_model['Churn']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Before SMOTE - Training set:\")\n",
    "print(f\"  Class 0: {(y_train == 0).sum()}\")\n",
    "print(f\"  Class 1: {(y_train == 1).sum()}\")\n",
    "\n",
    "# Apply SMOTE to training data only\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"\\nAfter SMOTE - Training set:\")\n",
    "print(f\"  Class 0: {(y_train_balanced == 0).sum()}\")\n",
    "print(f\"  Class 1: {(y_train_balanced == 1).sum()}\")\n",
    "\n",
    "# Train multiple models (Ensemble approach)\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    results[name] = {\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba,\n",
    "        'model': model\n",
    "    }\n",
    "    \n",
    "print(\"\\nModels trained successfully!\")\n",
    "```\n",
    "\n",
    "### Text Cell 9: Why Ensemble Methods?\n",
    "**Benefits of Using Multiple Models:**\n",
    "1. **Diversity**: Different algorithms capture different patterns\n",
    "2. **Robustness**: Reduces risk of overfitting to one model's bias\n",
    "3. **Improved Performance**: Averaging predictions often outperforms single models\n",
    "\n",
    "**Model Selection:**\n",
    "- **Logistic Regression**: Linear baseline, interpretable\n",
    "- **Random Forest**: Handles non-linear relationships, feature importance\n",
    "- **Gradient Boosting**: Often best performance, sequential learning\n",
    "\n",
    "### Code Cell 10: Evaluate Models\n",
    "```python\n",
    "# Evaluate each model\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, result in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(classification_report(y_test, result['predictions']))\n",
    "    print(f\"ROC-AUC Score: {roc_auc_score(y_test, result['probabilities']):.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, result['predictions'])\n",
    "    print(f\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "# Ensemble prediction (voting)\n",
    "ensemble_pred_proba = np.mean([\n",
    "    results['Logistic Regression']['probabilities'],\n",
    "    results['Random Forest']['probabilities'],\n",
    "    results['Gradient Boosting']['probabilities']\n",
    "], axis=0)\n",
    "ensemble_pred = (ensemble_pred_proba > 0.5).astype(int)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENSEMBLE MODEL (Voting)\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, ensemble_pred))\n",
    "print(f\"ROC-AUC Score: {roc_auc_score(y_test, ensemble_pred_proba):.4f}\")\n",
    "```\n",
    "\n",
    "### Code Cell 11: Feature Importance Analysis\n",
    "```python\n",
    "# Get feature importance from Random Forest\n",
    "rf_model = results['Random Forest']['model']\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 15 features\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features = feature_importance.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 15 Most Important Features (Random Forest)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10).to_string(index=False))\n",
    "```\n",
    "\n",
    "### Text Cell 10: Strategy Defense and Summary\n",
    "\n",
    "**Why This Strategy Enhances Performance:**\n",
    "\n",
    "**1. Feature Engineering (Addresses Domain Knowledge)**\n",
    "   - Creates features aligned with business understanding\n",
    "   - Captures complex relationships not visible in raw data\n",
    "   - Examples: Customer value (RevenuePerMonth), engagement (ServiceCount)\n",
    "\n",
    "**2. Class Balancing with SMOTE (Addresses Imbalance)**\n",
    "   - Churn datasets typically have imbalanced classes\n",
    "   - SMOTE creates synthetic minority samples intelligently\n",
    "   - Prevents model bias toward majority class\n",
    "   - Improves recall on churn (minority) class\n",
    "\n",
    "**3. Ensemble Methods (Addresses Model Variance)**\n",
    "   - Combines strengths of multiple algorithms\n",
    "   - Reduces overfitting through model diversity\n",
    "   - More robust predictions through voting/averaging\n",
    "   - Better generalization to unseen data\n",
    "\n",
    "**Expected Improvements:**\n",
    "- Better recall for churn class (fewer missed churners)\n",
    "- Higher ROC-AUC score (better class separation)\n",
    "- More balanced precision-recall trade-off\n",
    "- Feature importance for business insights\n",
    "\n",
    "**Trade-offs:**\n",
    "- Increased computational cost (multiple models)\n",
    "- More complex than single model approach\n",
    "- Requires careful hyperparameter tuning\n",
    "\n",
    "**Conclusion:**\n",
    "This three-pronged strategy addresses the key challenges in churn prediction:\n",
    "domain complexity, class imbalance, and model robustness. Each component\n",
    "contributes to improved overall performance.\n",
    "\n",
    "### Code Cell 12: Final Summary Statistics\n",
    "```python\n",
    "# Summary of improvements\n",
    "print(\"=\"*60)\n",
    "print(\"PERFORMANCE ENHANCEMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n1. Dataset:\")\n",
    "print(f\"   - Original size: 1000 samples\")\n",
    "print(f\"   - After outlier removal: {len(df_clean)} samples\")\n",
    "print(f\"   - Features after engineering: {X.shape[1]}\")\n",
    "\n",
    "print(f\"\\n2. Class Distribution:\")\n",
    "print(f\"   - Original (Test): {(y_test==0).sum()} non-churn, {(y_test==1).sum()} churn\")\n",
    "print(f\"   - Training (After SMOTE): {(y_train_balanced==0).sum()} each class\")\n",
    "\n",
    "print(f\"\\n3. Models Trained:\")\n",
    "for name in models.keys():\n",
    "    print(f\"   - {name}\")\n",
    "print(f\"   - Ensemble (Voting)\")\n",
    "\n",
    "print(f\"\\n4. Key Features Created:\")\n",
    "print(f\"   - RevenuePerMonth, ServiceCount, HasPremiumServices\")\n",
    "print(f\"   - ContractRisk, PaymentRisk, TenureCategory\")\n",
    "\n",
    "print(\"\\nStrategy successfully implemented!\")\n",
    "```\n",
    "üìù Implementation Notes:\n",
    "‚Ä¢ All code cells include comments explaining each step\n",
    "‚Ä¢ Text cells provide context and rationale for methods used\n",
    "‚Ä¢ Visualizations help understand outlier patterns\n",
    "‚Ä¢ Performance strategy is defended with clear reasoning\n",
    "‚Ä¢ Copy this content into Jupyter Notebook to execute"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
